# -*- coding: utf-8 -*-
"""core.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q-lo4vU9JUxJtQot2pd4QPa7CRUOSznH
"""



import json
import os
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
# Step 1: Load the dataset
def load_arxiv_data(file_path, max_entries=20000):
    data = []
    with open(file_path, 'r') as f:
        for i, line in tqdm(enumerate(f), total=max_entries, desc="Loading data"):
            if i >= max_entries:
                break
            paper = json.loads(line)
            if 'title' in paper and 'abstract' in paper:
                data.append({
                    'title': paper['title'],
                    'abstract': paper['abstract'],
                    'authors': paper.get('authors', 'N/A'),
                    'id': paper.get('id', 'N/A'),
                    'categories': paper.get('categories', 'N/A')
                })

    return data

# Step 4: Semantic search
def search(query, corpus_embeddings, model, data, top_k=5):
    query_embedding = model.encode([query])
    similarities = cosine_similarity(query_embedding, corpus_embeddings)[0]
    top_indices = similarities.argsort()[-top_k:][::-1]

    print("\nğŸ” Top Results:")
    for idx in top_indices:
        paper = data[idx]
        print("\n-------------------------------")
        print(f"ğŸ“„ Title: {paper['title']}")
        print(f"ğŸ‘¨â€ğŸ”¬ Authors: {paper['authors']}")
        print(f"ğŸ· Categories: {paper['categories']}")
        print(f"ğŸ“‘ Abstract: {paper['abstract'][:500]}...")
        print(f"ğŸ”— ID: {paper['id']}")

# Main Function
def main():
    # Load dataset
    json_path = '/content/drive/MyDrive/arxiv-metadata-oai-snapshot.json'
    if not os.path.exists(json_path):
        print("Dataset file not found. Please place 'arxiv-metadata.json' in the current directory.")
        return

    print("ğŸ”„ Loading and preparing data...")
    data = load_arxiv_data(json_path, max_entries=10000)
    corpus = prepare_corpus(data)

    print("ğŸ§  Creating embeddings...")
    corpus_embeddings, model = get_embeddings(corpus)

    # Search loop
    while True:
        query = input("\nEnter your search query (or type 'exit' to quit): ")
        if query.lower() == 'exit':
            break
        search(query, corpus_embeddings, model, data)

if __name__ == "__main__":
    main()

